{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2443cf9",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Generative Adversarial Networks, or GANs, are an architecture for training generative models, such as deep convolutional neural networks for generating images.\n",
    "\n",
    "The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.\n",
    "\n",
    "Two common types of layers that can be used in the generator model are a upsample layer (UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (Conv2DTranspose) that performs an inverse convolution operation.\n",
    "\n",
    "In this tutorial, you will discover how to use UpSampling2D and Conv2DTranspose Layers in Generative Adversarial Networks when generating images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416077c7",
   "metadata": {},
   "source": [
    "## Why Use Upsampling in Generative Adversarial Networks\n",
    "Generative Adversarial Networks are an architecture for neural networks for training a generative model. The architecture is comprised of a generator and a discriminator model, both of which are implemented as a deep convolutional neural network. The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). \n",
    "\n",
    "The generator is responsible for generating new plausible examples from the problem domain. The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.\n",
    "\n",
    "A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input.\n",
    "\n",
    "Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects. Often padding is used to counter this effect.\n",
    "\n",
    "The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate from coarse salient features to a more dense and detailed output.\n",
    "\n",
    "A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input.\n",
    "\n",
    "A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed convolutional layer.\n",
    "\n",
    "Both of these layers can be used on a GAN to perform the required upsampling operation to transform a small input into a large image output.\n",
    "\n",
    "In the following sections, we will take a closer look at each and develop an intuition for how they work so that we can use them effectively in our GAN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07370344",
   "metadata": {},
   "source": [
    "## How to Use the UpSampling2D Layer\n",
    "\n",
    "### Worked Example Using the UpSampling2D Layer\n",
    "Perhaps the simplest way to upsample an input is to double each row and column. The Keras deep learning library provides this capability in a layer called UpSampling2D. It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bf5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#model = Sequential()\n",
    "#model.add(UpSampling2D())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30755b5",
   "metadata": {},
   "source": [
    "We can demonstrate the behavior of this layer with a simple contrived example. First, we can define a contrived input image that is 2×2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19305ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input data\n",
    "#X = asarray([[1, 2],\n",
    "            #[3, 4]])\n",
    "\n",
    "# show input data for context\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f884e36",
   "metadata": {},
   "source": [
    "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd3ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input data into one sample a sample with a channel\n",
    "# X = X.reshape((1, 2, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d964b",
   "metadata": {},
   "source": [
    "We can now define our model. The model has only the UpSampling2D layer which takes 2×2 grayscale images as input directly and outputs the result of the upsampling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3691399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#model = Sequential()\n",
    "#model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
    "\n",
    "# summarize the model\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89883807",
   "metadata": {},
   "source": [
    "We can then use the model to make a prediction, that is upsample a provided input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5065d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the model\n",
    "# yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c12ba8",
   "metadata": {},
   "source": [
    "The output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5b80d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape output to remove channel to make printing easier\n",
    "#yhat = yhat.reshape((4, 4))\n",
    "\n",
    "# summarize output\n",
    "#print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1833374",
   "metadata": {},
   "source": [
    "aaaaand tying all of this together, the complete example of using the UpSampling2D layer in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211f145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "up_sampling2d (UpSampling2D  (None, 4, 4, 1)           0         \n",
      ")                                                                \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[1. 1. 2. 2.]\n",
      " [1. 1. 2. 2.]\n",
      " [3. 3. 4. 4.]\n",
      " [3. 3. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# example of using the upsampling layer\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import UpSampling2D\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "\t\t\t [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# make a prediction with the model\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f114374",
   "metadata": {},
   "source": [
    "Running the example first creates and summarizes our 2×2 input data.\n",
    "\n",
    "Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input.\n",
    "\n",
    "Finally, the model is used to upsample our input, resulting in a doubling of each row and column for our input data, as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546876be",
   "metadata": {},
   "source": [
    "By default, the UpSampling2D will double each input dimension. This is defined by the ‘size‘ argument that is set to the tuple (2,2).\n",
    "\n",
    "You may want to use different factors on each dimension, such as double the width and triple the height. This could be achieved by setting the ‘size‘ argument to (2, 3). The result of applying this operation to a 2×2 image would be a 4×6 output image (e.g. 2×2 and 2×3).\n",
    "\n",
    "Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the ‘interpolation‘ argument set to ‘nearest‘."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca15d3",
   "metadata": {},
   "source": [
    "### Simple Generator Model With the UpSampling2D Layer\n",
    "The UpSampling2D layer is simple and effective, although does not perform any learning.\n",
    "\n",
    "It is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.\n",
    "\n",
    "We can demonstrate this with an example:\n",
    "\n",
    "- In this case, our little GAN generator model must produce a 10×10 image and take a 100 element vector from the latent space as input.\n",
    "\n",
    "- First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.\n",
    "\n",
    "- Next, the 5×5 feature maps can be upsampled to a 10×10 feature map.\n",
    "\n",
    "- Finally, the upsampled feature maps can be interpreted and filled in with hopefully useful detail by a Conv2D layer.\n",
    "\n",
    "The Conv2D has a single feature map as output to create the single image we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009ac9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3200)              323200    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling  (None, 10, 10, 128)       0         \n",
      "2D)                                                              \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 10, 10, 1)         1153      \n",
      "=================================================================\n",
      "Total params: 324,353\n",
      "Trainable params: 324,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of using upsampling in a simple generator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Conv2D\n",
    "# define model\n",
    "model = Sequential()\n",
    "# define input shape, output enough activations for for 128 5x5 image\n",
    "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
    "# reshape vector of activations into 128 feature maps with 5x5\n",
    "model.add(Reshape((5, 5, 128)))\n",
    "# double input from 128 5x5 to 1 10x10 feature map\n",
    "model.add(UpSampling2D())\n",
    "# fill in detail in the upsampled feature maps and output a single image\n",
    "model.add(Conv2D(1, (3,3), padding='same'))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223d01c",
   "metadata": {},
   "source": [
    "Running the example creates the model and summarizes the output shape of each layer.\n",
    "\n",
    "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5. The widths and heights are doubled to 10×10 by the UpSampling2D layer, resulting in a feature map with quadruple the area. Finally, the Conv2D processes these feature maps and adds in detail, outputting a single 10×10 image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8134792",
   "metadata": {},
   "source": [
    "## How to Use the Conv2DTranspose Layer\n",
    "The Conv2DTranspose or transpose convolutional layer is more complex than a simple upsampling layer.\n",
    "\n",
    "A simple way to think about it is that it both performs the upsample operation and interprets the coarse input data to fill in the detail while it is upsampling. It is like a layer that combines the UpSampling2D and Conv2D layers into one layer. This is a crude understanding, but a practical starting point.\n",
    "\n",
    "The transpose convolutional layer performs an inverse convolution operation. Specifically, the forward and backward passes of the convolutional layer are reversed. It is sometimes called a deconvolution or deconvolutional layer and models that use these layers can be referred to as deconvolutional networks, or deconvnets.It is a very flexible layer, although we will focus on its use in the generative models from upsampling an input image.\n",
    "\n",
    "The transpose convolutional layer is much like a normal convolutional layer. It requires that you specify the number of filters and the kernel size of each filter. The key to the layer is the stride.\n",
    "\n",
    "Typically, the stride of a convolutional layer is (1×1), that is a filter is moved along one pixel horizontally for each read from left-to-right, then down pixel for the next row of reads. A stride of 2×2 on a normal convolutional layer has the effect of downsampling the input, much like a pooling layer. In fact, a 2×2 stride can be used instead of a pooling layer in the discriminator model.\n",
    "\n",
    "The transpose convolutional layer is like an inverse convolutional layer. As such, you would intuitively think that a 2×2 stride would upsample the input instead of downsample, which is exactly what happens.\n",
    "\n",
    "Stride or strides refers to the manner of a filter scanning across an input in a traditional convolutional layer. Whereas, in a transpose convolutional layer, stride refers to the manner in which outputs in the feature map are laid down. This effect can be implemented with a normal convolutional layer using a fractional input stride (f), e.g. with a stride of f=1/2. When inverted, the output stride is set to the numerator of this fraction, e.g. f=2. One way that this effect can be achieved with a normal convolutional layer is by inserting new rows and columns of 0.0 values in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4e2e1",
   "metadata": {},
   "source": [
    "### Worked Example Using the Conv2DTranspose Layer\n",
    "We can demonstrate the behavior of this layer with a simple contrived example.\n",
    "\n",
    "- First, we can define a contrived input image that is 2×2 pixels, as we did in the previous section. We can use specific values for each pixel so that after the transpose convolutional operation, we can see exactly what effect the operation had on the input.\n",
    "\n",
    "- Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.\n",
    "\n",
    "- We can now define our model. The model has only the Conv2DTranspose layer, which takes 2×2 grayscale images as input directly and outputs the result of the operation.\n",
    "\n",
    "    - The Conv2DTranspose both upsamples and performs a convolution. As such, we must specify both the number of filters and the size of the filters as we do for Conv2D layers. Additionally, we must specify a stride of (2,2) because the upsampling is achieved by the stride behavior of the convolution on the input.\n",
    "\n",
    "    - Specifying a stride of (2,2) has the effect of spacing out the input. Specifically, rows and columns of 0.0 values are inserted to achieve the desired stride.\n",
    "\n",
    "    - In this example, we will use one filter, with a 1×1 kernel and a stride of 2×2 so that the 2×2 input image is upsampled to 4×4.\n",
    "\n",
    "- To make it clear what the Conv2DTranspose layer is doing, we will fix the single weight in the single filter to the value of 1.0 and use a bias value of 0.0. These weights, along with a kernel size of (1,1) will mean that values in the input will be multiplied by 1 and output as-is, and the 0 values in the new rows and columns added via the stride of 2×2 will be output as 0 (e.g. 1 * 0 in each case).\n",
    "- We can then use the model to make a prediction, that is upsample a provided input image.\n",
    "- The output will have four dimensions, like the input, therefore, we can convert it back to a 2×2 array to make it easier to review the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b78c70",
   "metadata": {},
   "source": [
    "Tying all of this together, the complete example of using the Conv2DTranspose layer in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a996ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_transpose (Conv2DTra  (None, 4, 4, 1)           2         \n",
      "nspose)                                                          \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[1. 0. 2. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 0. 4. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# example of using the transpose convolutional layer\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2DTranspose\n",
    "# define input data\n",
    "X = asarray([[1, 2],\n",
    "\t\t\t [3, 4]])\n",
    "# show input data for context\n",
    "print(X)\n",
    "# reshape input data into one sample a sample with a channel\n",
    "X = X.reshape((1, 2, 2, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1)))\n",
    "# summarize the model\n",
    "model.summary()\n",
    "# define weights that they do nothing\n",
    "weights = [asarray([[[[1]]]]), asarray([0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)\n",
    "# make a prediction with the model\n",
    "yhat = model.predict(X)\n",
    "# reshape output to remove channel to make printing easier\n",
    "yhat = yhat.reshape((4, 4))\n",
    "# summarize output\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b048d42",
   "metadata": {},
   "source": [
    "Running the example first creates and summarizes our 2×2 input data.\n",
    "\n",
    "Next, the model is summarized. We can see that it will output a 4×4 result as we expect, and importantly, the layer two parameters or model weights. One for the single 1×1 filter and one for the bias. Unlike the UpSampling2D layer, the Conv2DTranspose will learn during training and will attempt to fill in detail as part of the upsampling process.\n",
    "\n",
    "Finally, the model is used to upsample our input. We can see that the calculations of the cells that involve real values as input result in the real value as output (e.g. 1×1, 1×2, etc.). We can see that where new rows and columns have been inserted by the stride of 2×2, that their 0.0 values multiplied by the 1.0 values in the single 1×1 filter have resulted in 0 values in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e317d7",
   "metadata": {},
   "source": [
    "### Simple Generator Model With the Conv2DTranspose Layer\n",
    "he Conv2DTranspose is more complex than the UpSampling2D layer, but it is also effective when used in GAN models, specifically the generator model.\n",
    "\n",
    "Either approach can be used, although the Conv2DTranspose layer is preferred, perhaps because of the simpler generator models and possibly better results, although GAN performance and skill is notoriously difficult to quantify.\n",
    "\n",
    "We can demonstrate using the Conv2DTranspose layer in a generator model with another simple example.\n",
    "\n",
    "- In this case, our little GAN generator model must produce a 10×10 image and take a 100-element vector from the latent space as input, as in the previous UpSampling2D example.\n",
    "\n",
    "- First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5×5 image.\n",
    "- Next, the 5×5 feature maps can be upsampled to a 10×10 feature map.\n",
    "    - We will use a 3×3 kernel size for the single filter, which will result in a slightly larger than doubled width and height in the output feature map (11×11).\n",
    "    - Therefore, we will set ‘padding‘ to ‘same’ to ensure the output dimensions are 10×10 as required.\n",
    "\n",
    "Tying this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de48410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3200)              323200    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DT  (None, 10, 10, 1)         1153      \n",
      "ranspose)                                                        \n",
      "=================================================================\n",
      "Total params: 324,353\n",
      "Trainable params: 324,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# example of using transpose conv in a simple generator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Conv2D\n",
    "# define model\n",
    "model = Sequential()\n",
    "# define input shape, output enough activations for for 128 5x5 image\n",
    "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
    "# reshape vector of activations into 128 feature maps with 5x5\n",
    "model.add(Reshape((5, 5, 128)))\n",
    "# double input from 128 5x5 to 1 10x10 feature map\n",
    "model.add(Conv2DTranspose(1, (3,3), strides=(2,2), padding='same'))\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e49cc9",
   "metadata": {},
   "source": [
    "Running the example creates the model and summarizes the output shape of each layer.\n",
    "\n",
    "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5.\n",
    "\n",
    "The widths and heights are doubled to 10×10 by the Conv2DTranspose layer resulting in a single feature map with quadruple the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f7652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
